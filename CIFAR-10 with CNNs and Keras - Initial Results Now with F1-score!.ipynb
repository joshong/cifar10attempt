{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing CIFAR-10 using Convolutional Neural Networks - Initial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import LocallyConnected2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the CIFAR-10 dataset anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's a dataset that consists of 60000 32x32 colored images divided into 10 classes that correspond to 6000 images each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/cifar10.png\\\" alt=\"CIFAR-10\\\" style=\"width: 75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our goal right now is to train a Convolutional Neural Network that could accurately classify these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start with a basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial model will consist of a Conv layer, followed by RELU activation layer, max pooling, dropout and then fully connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      " - 50s - loss: 1.6831 - acc: 0.3767 - val_loss: 1.3390 - val_acc: 0.5159\n",
      "Epoch 2/40\n",
      " - 40s - loss: 1.2544 - acc: 0.5522 - val_loss: 1.0607 - val_acc: 0.6205\n",
      "Epoch 3/40\n",
      " - 40s - loss: 1.0881 - acc: 0.6140 - val_loss: 0.9500 - val_acc: 0.6635\n",
      "Epoch 4/40\n",
      " - 41s - loss: 0.9929 - acc: 0.6462 - val_loss: 0.9281 - val_acc: 0.6737\n",
      "Epoch 5/40\n",
      " - 41s - loss: 0.9184 - acc: 0.6752 - val_loss: 0.8413 - val_acc: 0.7114\n",
      "Epoch 6/40\n",
      " - 41s - loss: 0.8586 - acc: 0.6974 - val_loss: 0.8201 - val_acc: 0.7199\n",
      "Epoch 7/40\n",
      " - 40s - loss: 0.8026 - acc: 0.7185 - val_loss: 0.8033 - val_acc: 0.7217\n",
      "Epoch 8/40\n",
      " - 40s - loss: 0.7597 - acc: 0.7324 - val_loss: 0.7478 - val_acc: 0.7419\n",
      "Epoch 9/40\n",
      " - 40s - loss: 0.7187 - acc: 0.7462 - val_loss: 0.7601 - val_acc: 0.7393\n",
      "Epoch 10/40\n",
      " - 40s - loss: 0.6810 - acc: 0.7609 - val_loss: 0.7212 - val_acc: 0.7480\n",
      "Epoch 11/40\n",
      " - 41s - loss: 0.6508 - acc: 0.7710 - val_loss: 0.7128 - val_acc: 0.7586\n",
      "Epoch 12/40\n",
      " - 41s - loss: 0.6289 - acc: 0.7779 - val_loss: 0.6953 - val_acc: 0.7639\n",
      "Epoch 13/40\n",
      " - 40s - loss: 0.5986 - acc: 0.7884 - val_loss: 0.6856 - val_acc: 0.7634\n",
      "Epoch 14/40\n",
      " - 40s - loss: 0.5808 - acc: 0.7955 - val_loss: 0.6778 - val_acc: 0.7710\n",
      "Epoch 15/40\n",
      " - 40s - loss: 0.5605 - acc: 0.8035 - val_loss: 0.6534 - val_acc: 0.7821\n",
      "Epoch 16/40\n",
      " - 40s - loss: 0.5409 - acc: 0.8086 - val_loss: 0.6766 - val_acc: 0.7703\n",
      "Epoch 17/40\n",
      " - 40s - loss: 0.5222 - acc: 0.8160 - val_loss: 0.6466 - val_acc: 0.7787\n",
      "Epoch 18/40\n",
      " - 40s - loss: 0.5093 - acc: 0.8223 - val_loss: 0.6672 - val_acc: 0.7762\n",
      "Epoch 19/40\n",
      " - 40s - loss: 0.4919 - acc: 0.8251 - val_loss: 0.6662 - val_acc: 0.7769\n",
      "Epoch 20/40\n",
      " - 40s - loss: 0.4801 - acc: 0.8321 - val_loss: 0.6464 - val_acc: 0.7827\n",
      "Epoch 21/40\n",
      " - 41s - loss: 0.4674 - acc: 0.8355 - val_loss: 0.6675 - val_acc: 0.7793\n",
      "Epoch 22/40\n",
      " - 41s - loss: 0.4576 - acc: 0.8404 - val_loss: 0.6845 - val_acc: 0.7698\n",
      "Epoch 23/40\n",
      " - 41s - loss: 0.4404 - acc: 0.8475 - val_loss: 0.6904 - val_acc: 0.7688\n",
      "Epoch 24/40\n",
      " - 41s - loss: 0.4343 - acc: 0.8476 - val_loss: 0.6480 - val_acc: 0.7859\n",
      "Epoch 25/40\n",
      " - 41s - loss: 0.4205 - acc: 0.8539 - val_loss: 0.6418 - val_acc: 0.7896\n",
      "Epoch 26/40\n",
      " - 41s - loss: 0.4103 - acc: 0.8576 - val_loss: 0.6549 - val_acc: 0.7837\n",
      "Epoch 27/40\n",
      " - 41s - loss: 0.4013 - acc: 0.8617 - val_loss: 0.6554 - val_acc: 0.7888\n",
      "Epoch 28/40\n",
      " - 41s - loss: 0.3907 - acc: 0.8618 - val_loss: 0.6701 - val_acc: 0.7777\n",
      "Epoch 29/40\n",
      " - 41s - loss: 0.3829 - acc: 0.8680 - val_loss: 0.6454 - val_acc: 0.7906\n",
      "Epoch 30/40\n",
      " - 40s - loss: 0.3785 - acc: 0.8696 - val_loss: 0.6353 - val_acc: 0.7945\n",
      "Epoch 31/40\n",
      " - 40s - loss: 0.3615 - acc: 0.8744 - val_loss: 0.6449 - val_acc: 0.7908\n",
      "Epoch 32/40\n",
      " - 40s - loss: 0.3644 - acc: 0.8747 - val_loss: 0.6683 - val_acc: 0.7848\n",
      "Epoch 33/40\n",
      " - 40s - loss: 0.3609 - acc: 0.8752 - val_loss: 0.6658 - val_acc: 0.7896\n",
      "Epoch 34/40\n",
      " - 40s - loss: 0.3497 - acc: 0.8786 - val_loss: 0.6965 - val_acc: 0.7818\n",
      "Epoch 35/40\n",
      " - 40s - loss: 0.3472 - acc: 0.8790 - val_loss: 0.6729 - val_acc: 0.7864\n",
      "Epoch 36/40\n",
      "\n",
      "Epoch 00036: reducing learning rate to 0.0005000000237487257.\n",
      " - 41s - loss: 0.3353 - acc: 0.8854 - val_loss: 0.6643 - val_acc: 0.7930\n",
      "Epoch 37/40\n",
      " - 41s - loss: 0.2583 - acc: 0.9093 - val_loss: 0.6520 - val_acc: 0.8027\n",
      "Epoch 38/40\n",
      " - 41s - loss: 0.2422 - acc: 0.9162 - val_loss: 0.6434 - val_acc: 0.8035\n",
      "Epoch 39/40\n",
      " - 41s - loss: 0.2263 - acc: 0.9213 - val_loss: 0.6682 - val_acc: 0.8009\n",
      "Epoch 40/40\n",
      " - 41s - loss: 0.2214 - acc: 0.9232 - val_loss: 0.6509 - val_acc: 0.8068\n",
      "Test loss: 0.650881683636\n",
      "Test accuracy: 0.8068\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (2, 2), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(128, (2, 2), activation='relu'))\n",
    "model.add(Conv2D(128, (1, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(256, (2, 2), activation='relu'))\n",
    "model.add(Conv2D(256, (2, 2), activation='relu'))\n",
    "model.add(Conv2D(256, (1, 1), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, batch_size=32, verbose=2, callbacks=[plateau])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      " - 43s - loss: 1.4515 - acc: 0.4857 - val_loss: 1.2864 - val_acc: 0.5369\n",
      "Epoch 2/60\n",
      " - 40s - loss: 1.1525 - acc: 0.5941 - val_loss: 1.0895 - val_acc: 0.6122\n",
      "Epoch 3/60\n",
      " - 40s - loss: 1.0452 - acc: 0.6327 - val_loss: 0.9408 - val_acc: 0.6711\n",
      "Epoch 4/60\n",
      " - 40s - loss: 0.9745 - acc: 0.6597 - val_loss: 0.9429 - val_acc: 0.6725\n",
      "Epoch 5/60\n",
      " - 41s - loss: 0.9365 - acc: 0.6746 - val_loss: 0.8660 - val_acc: 0.7028\n",
      "Epoch 6/60\n",
      " - 41s - loss: 0.8896 - acc: 0.6921 - val_loss: 0.9259 - val_acc: 0.6762\n",
      "Epoch 7/60\n",
      " - 40s - loss: 0.8626 - acc: 0.7020 - val_loss: 0.8487 - val_acc: 0.7086\n",
      "Epoch 8/60\n",
      " - 40s - loss: 0.8396 - acc: 0.7136 - val_loss: 0.8336 - val_acc: 0.7100\n",
      "Epoch 9/60\n",
      " - 41s - loss: 0.8160 - acc: 0.7193 - val_loss: 0.7996 - val_acc: 0.7211\n",
      "Epoch 10/60\n",
      " - 41s - loss: 0.7958 - acc: 0.7294 - val_loss: 0.7846 - val_acc: 0.7371\n",
      "Epoch 11/60\n",
      " - 41s - loss: 0.7749 - acc: 0.7366 - val_loss: 0.8158 - val_acc: 0.7260\n",
      "Epoch 12/60\n",
      " - 41s - loss: 0.7609 - acc: 0.7415 - val_loss: 0.8121 - val_acc: 0.7223\n",
      "Epoch 13/60\n",
      " - 41s - loss: 0.7408 - acc: 0.7509 - val_loss: 0.7986 - val_acc: 0.7319\n",
      "Epoch 14/60\n",
      " - 41s - loss: 0.7215 - acc: 0.7541 - val_loss: 0.7493 - val_acc: 0.7428\n",
      "Epoch 15/60\n",
      " - 41s - loss: 0.7079 - acc: 0.7618 - val_loss: 0.7786 - val_acc: 0.7331\n",
      "Epoch 16/60\n",
      " - 41s - loss: 0.6934 - acc: 0.7678 - val_loss: 0.7691 - val_acc: 0.7416\n",
      "Epoch 17/60\n",
      " - 41s - loss: 0.6859 - acc: 0.7705 - val_loss: 0.7628 - val_acc: 0.7392\n",
      "Epoch 18/60\n",
      " - 41s - loss: 0.6622 - acc: 0.7796 - val_loss: 0.7591 - val_acc: 0.7532\n",
      "Epoch 19/60\n",
      " - 40s - loss: 0.6546 - acc: 0.7813 - val_loss: 0.7370 - val_acc: 0.7517\n",
      "Epoch 20/60\n",
      " - 40s - loss: 0.6411 - acc: 0.7872 - val_loss: 0.7454 - val_acc: 0.7591\n",
      "Epoch 21/60\n",
      " - 40s - loss: 0.6386 - acc: 0.7886 - val_loss: 0.7823 - val_acc: 0.7556\n",
      "Epoch 22/60\n",
      " - 40s - loss: 0.6269 - acc: 0.7959 - val_loss: 0.7381 - val_acc: 0.7651\n",
      "Epoch 23/60\n",
      " - 40s - loss: 0.6103 - acc: 0.7999 - val_loss: 0.7290 - val_acc: 0.7683\n",
      "Epoch 24/60\n",
      " - 40s - loss: 0.5955 - acc: 0.8059 - val_loss: 0.7615 - val_acc: 0.7578\n",
      "Epoch 25/60\n",
      " - 40s - loss: 0.6030 - acc: 0.8041 - val_loss: 0.7802 - val_acc: 0.7418\n",
      "Epoch 26/60\n",
      " - 40s - loss: 0.5908 - acc: 0.8093 - val_loss: 0.7096 - val_acc: 0.7675\n",
      "Epoch 27/60\n",
      " - 40s - loss: 0.5681 - acc: 0.8158 - val_loss: 0.7437 - val_acc: 0.7637\n",
      "Epoch 28/60\n",
      " - 42s - loss: 0.5637 - acc: 0.8167 - val_loss: 0.8276 - val_acc: 0.7477\n",
      "Epoch 29/60\n",
      " - 40s - loss: 0.5644 - acc: 0.8197 - val_loss: 0.8158 - val_acc: 0.7475\n",
      "Epoch 30/60\n",
      " - 40s - loss: 0.5435 - acc: 0.8247 - val_loss: 0.7295 - val_acc: 0.7746\n",
      "Epoch 31/60\n",
      " - 40s - loss: 0.5609 - acc: 0.8200 - val_loss: 0.7583 - val_acc: 0.7652\n",
      "Epoch 32/60\n",
      "\n",
      "Epoch 00032: reducing learning rate to 0.0005000000237487257.\n",
      " - 109s - loss: 0.5460 - acc: 0.8260 - val_loss: 0.7713 - val_acc: 0.7684\n",
      "Epoch 33/60\n",
      " - 40s - loss: 0.4063 - acc: 0.8668 - val_loss: 0.6846 - val_acc: 0.7889\n",
      "Epoch 34/60\n",
      " - 40s - loss: 0.3776 - acc: 0.8784 - val_loss: 0.6874 - val_acc: 0.7900\n",
      "Epoch 35/60\n",
      " - 40s - loss: 0.3530 - acc: 0.8853 - val_loss: 0.6626 - val_acc: 0.7953\n",
      "Epoch 36/60\n",
      " - 40s - loss: 0.3381 - acc: 0.8901 - val_loss: 0.6784 - val_acc: 0.7976\n",
      "Epoch 37/60\n",
      " - 40s - loss: 0.3400 - acc: 0.8901 - val_loss: 0.7290 - val_acc: 0.7912\n",
      "Epoch 38/60\n",
      " - 40s - loss: 0.3174 - acc: 0.8961 - val_loss: 0.6937 - val_acc: 0.7910\n",
      "Epoch 39/60\n",
      " - 40s - loss: 0.3086 - acc: 0.8982 - val_loss: 0.7144 - val_acc: 0.7931\n",
      "Epoch 40/60\n",
      " - 40s - loss: 0.3066 - acc: 0.8992 - val_loss: 0.6492 - val_acc: 0.7965\n",
      "Epoch 41/60\n",
      " - 39s - loss: 0.2993 - acc: 0.9017 - val_loss: 0.7304 - val_acc: 0.7861\n",
      "Epoch 42/60\n",
      " - 39s - loss: 0.2948 - acc: 0.9027 - val_loss: 0.6881 - val_acc: 0.8021\n",
      "Epoch 43/60\n",
      " - 39s - loss: 0.2821 - acc: 0.9088 - val_loss: 0.7194 - val_acc: 0.7879\n",
      "Epoch 44/60\n",
      " - 39s - loss: 0.2819 - acc: 0.9084 - val_loss: 0.7222 - val_acc: 0.7988\n",
      "Epoch 45/60\n",
      " - 39s - loss: 0.2731 - acc: 0.9105 - val_loss: 0.6899 - val_acc: 0.8013\n",
      "Epoch 46/60\n",
      "\n",
      "Epoch 00046: reducing learning rate to 0.0002500000118743628.\n",
      " - 40s - loss: 0.2788 - acc: 0.9099 - val_loss: 0.6559 - val_acc: 0.8028\n",
      "Epoch 47/60\n",
      " - 39s - loss: 0.2198 - acc: 0.9272 - val_loss: 0.6770 - val_acc: 0.8081\n",
      "Epoch 48/60\n",
      " - 39s - loss: 0.2082 - acc: 0.9324 - val_loss: 0.6662 - val_acc: 0.8089\n",
      "Epoch 49/60\n",
      " - 39s - loss: 0.1935 - acc: 0.9360 - val_loss: 0.6845 - val_acc: 0.8073\n",
      "Epoch 50/60\n",
      " - 40s - loss: 0.1964 - acc: 0.9362 - val_loss: 0.6788 - val_acc: 0.8088\n",
      "Epoch 51/60\n",
      "\n",
      "Epoch 00051: reducing learning rate to 0.0001250000059371814.\n",
      " - 39s - loss: 0.1883 - acc: 0.9389 - val_loss: 0.6624 - val_acc: 0.8079\n",
      "Epoch 52/60\n",
      " - 39s - loss: 0.1701 - acc: 0.9435 - val_loss: 0.6592 - val_acc: 0.8129\n",
      "Epoch 53/60\n",
      " - 40s - loss: 0.1583 - acc: 0.9465 - val_loss: 0.6644 - val_acc: 0.8151\n",
      "Epoch 54/60\n",
      " - 41s - loss: 0.1551 - acc: 0.9484 - val_loss: 0.6614 - val_acc: 0.8147\n",
      "Epoch 55/60\n",
      " - 41s - loss: 0.1527 - acc: 0.9488 - val_loss: 0.6671 - val_acc: 0.8120\n",
      "Epoch 56/60\n",
      "\n",
      "Epoch 00056: reducing learning rate to 6.25000029685907e-05.\n",
      " - 41s - loss: 0.1462 - acc: 0.9512 - val_loss: 0.6599 - val_acc: 0.8148\n",
      "Epoch 57/60\n",
      " - 40s - loss: 0.1382 - acc: 0.9547 - val_loss: 0.7014 - val_acc: 0.8133\n",
      "Epoch 58/60\n",
      " - 40s - loss: 0.1366 - acc: 0.9548 - val_loss: 0.6690 - val_acc: 0.8137\n",
      "Epoch 59/60\n",
      " - 41s - loss: 0.1363 - acc: 0.9543 - val_loss: 0.6790 - val_acc: 0.8138\n",
      "Epoch 60/60\n",
      " - 41s - loss: 0.1299 - acc: 0.9564 - val_loss: 0.6795 - val_acc: 0.8128\n",
      "Test loss: 0.679545882416\n",
      "Test accuracy: 0.8128\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (2, 2), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(128, (2, 2), activation='elu'))\n",
    "model.add(Conv2D(128, (1, 1), activation='elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(256, (2, 2), activation='elu'))\n",
    "model.add(Conv2D(256, (2, 2), activation='elu'))\n",
    "model.add(Conv2D(256, (1, 1), activation='elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=60, batch_size=32, verbose=2, callbacks=[plateau])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taken from https://stackoverflow.com/questions/45411902/how-to-use-f1-score-with-keras-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/60\n",
      " - 49s - loss: 1.4735 - acc: 0.4760 - f1_score: nan - val_loss: 1.1988 - val_acc: 0.5719 - val_f1_score: 0.4910\n",
      "Epoch 2/60\n",
      " - 42s - loss: 1.1734 - acc: 0.5877 - f1_score: 0.5534 - val_loss: 1.0143 - val_acc: 0.6433 - val_f1_score: 0.5977\n",
      "Epoch 3/60\n",
      " - 42s - loss: 1.0571 - acc: 0.6319 - f1_score: 0.6055 - val_loss: 1.0358 - val_acc: 0.6393 - val_f1_score: 0.6292\n",
      "Epoch 4/60\n",
      " - 42s - loss: 0.9882 - acc: 0.6535 - f1_score: 0.6364 - val_loss: 0.9558 - val_acc: 0.6682 - val_f1_score: 0.6429\n",
      "Epoch 5/60\n",
      " - 42s - loss: 0.9398 - acc: 0.6710 - f1_score: 0.6561 - val_loss: 0.8672 - val_acc: 0.7016 - val_f1_score: 0.6749\n",
      "Epoch 6/60\n",
      " - 42s - loss: 0.9118 - acc: 0.6821 - f1_score: 0.6727 - val_loss: 0.8713 - val_acc: 0.6947 - val_f1_score: 0.6726\n",
      "Epoch 7/60\n",
      " - 42s - loss: 0.8779 - acc: 0.6941 - f1_score: 0.6843 - val_loss: 0.8263 - val_acc: 0.7109 - val_f1_score: 0.7036\n",
      "Epoch 8/60\n",
      " - 42s - loss: 0.8460 - acc: 0.7062 - f1_score: 0.6991 - val_loss: 0.8160 - val_acc: 0.7192 - val_f1_score: 0.7077\n",
      "Epoch 9/60\n",
      " - 41s - loss: 0.8208 - acc: 0.7179 - f1_score: 0.7105 - val_loss: 0.8566 - val_acc: 0.7098 - val_f1_score: 0.6945\n",
      "Epoch 10/60\n",
      " - 41s - loss: 0.8074 - acc: 0.7224 - f1_score: 0.7170 - val_loss: 0.7927 - val_acc: 0.7366 - val_f1_score: 0.7295\n",
      "Epoch 11/60\n",
      " - 41s - loss: 0.7841 - acc: 0.7315 - f1_score: 0.7261 - val_loss: 0.7676 - val_acc: 0.7380 - val_f1_score: 0.7297\n",
      "Epoch 12/60\n",
      " - 41s - loss: 0.7710 - acc: 0.7371 - f1_score: 0.7331 - val_loss: 0.8164 - val_acc: 0.7274 - val_f1_score: 0.7218\n",
      "Epoch 13/60\n",
      " - 41s - loss: 0.7506 - acc: 0.7482 - f1_score: 0.7432 - val_loss: 0.7672 - val_acc: 0.7416 - val_f1_score: 0.7354\n",
      "Epoch 14/60\n",
      " - 41s - loss: 0.7377 - acc: 0.7510 - f1_score: 0.7474 - val_loss: 0.7997 - val_acc: 0.7319 - val_f1_score: 0.7190\n",
      "Epoch 15/60\n",
      " - 41s - loss: 0.7227 - acc: 0.7557 - f1_score: 0.7529 - val_loss: 0.8131 - val_acc: 0.7239 - val_f1_score: 0.7178\n",
      "Epoch 16/60\n",
      " - 41s - loss: 0.7025 - acc: 0.7626 - f1_score: 0.7598 - val_loss: 0.7995 - val_acc: 0.7371 - val_f1_score: 0.7317\n",
      "Epoch 17/60\n",
      " - 41s - loss: 0.6884 - acc: 0.7698 - f1_score: 0.7673 - val_loss: 0.7471 - val_acc: 0.7543 - val_f1_score: 0.7534\n",
      "Epoch 18/60\n",
      " - 42s - loss: 0.6825 - acc: 0.7701 - f1_score: 0.7690 - val_loss: 0.7814 - val_acc: 0.7356 - val_f1_score: 0.7277\n",
      "Epoch 19/60\n",
      " - 42s - loss: 0.6769 - acc: 0.7726 - f1_score: 0.7709 - val_loss: 0.7227 - val_acc: 0.7621 - val_f1_score: 0.7619\n",
      "Epoch 20/60\n",
      " - 41s - loss: 0.6511 - acc: 0.7846 - f1_score: 0.7839 - val_loss: 0.7702 - val_acc: 0.7562 - val_f1_score: 0.7516\n",
      "Epoch 21/60\n",
      " - 40s - loss: 0.6490 - acc: 0.7852 - f1_score: 0.7845 - val_loss: 0.7462 - val_acc: 0.7582 - val_f1_score: 0.7520\n",
      "Epoch 22/60\n",
      " - 41s - loss: 0.6333 - acc: 0.7915 - f1_score: 0.7914 - val_loss: 0.7700 - val_acc: 0.7567 - val_f1_score: 0.7576\n",
      "Epoch 23/60\n",
      " - 41s - loss: 0.6218 - acc: 0.7924 - f1_score: 0.7923 - val_loss: 0.7675 - val_acc: 0.7513 - val_f1_score: 0.7504\n",
      "Epoch 24/60\n",
      " - 41s - loss: 0.6320 - acc: 0.7929 - f1_score: 0.7945 - val_loss: 0.7313 - val_acc: 0.7597 - val_f1_score: 0.7592\n",
      "Epoch 25/60\n",
      "\n",
      "Epoch 00025: reducing learning rate to 0.0005000000237487257.\n",
      " - 43s - loss: 0.6105 - acc: 0.7991 - f1_score: 0.7998 - val_loss: 0.7283 - val_acc: 0.7688 - val_f1_score: 0.7719\n",
      "Epoch 26/60\n",
      " - 41s - loss: 0.4661 - acc: 0.8459 - f1_score: 0.8470 - val_loss: 0.6908 - val_acc: 0.7859 - val_f1_score: 0.7874\n",
      "Epoch 27/60\n",
      " - 41s - loss: 0.4261 - acc: 0.8564 - f1_score: 0.8581 - val_loss: 0.6806 - val_acc: 0.7881 - val_f1_score: 0.7913\n",
      "Epoch 28/60\n",
      " - 41s - loss: 0.4107 - acc: 0.8629 - f1_score: 0.8651 - val_loss: 0.6855 - val_acc: 0.7846 - val_f1_score: 0.7884\n",
      "Epoch 29/60\n",
      " - 41s - loss: 0.3886 - acc: 0.8701 - f1_score: 0.8713 - val_loss: 0.6735 - val_acc: 0.7961 - val_f1_score: 0.8012\n",
      "Epoch 30/60\n",
      " - 41s - loss: 0.3860 - acc: 0.8703 - f1_score: 0.8718 - val_loss: 0.6825 - val_acc: 0.7817 - val_f1_score: 0.7883\n",
      "Epoch 31/60\n",
      " - 40s - loss: 0.3727 - acc: 0.8763 - f1_score: 0.8775 - val_loss: 0.6860 - val_acc: 0.7892 - val_f1_score: 0.7916\n",
      "Epoch 32/60\n",
      " - 40s - loss: 0.3561 - acc: 0.8818 - f1_score: 0.8834 - val_loss: 0.6953 - val_acc: 0.7837 - val_f1_score: 0.7840\n",
      "Epoch 33/60\n",
      " - 41s - loss: 0.3486 - acc: 0.8823 - f1_score: 0.8838 - val_loss: 0.6965 - val_acc: 0.7897 - val_f1_score: 0.7939\n",
      "Epoch 34/60\n",
      " - 41s - loss: 0.3412 - acc: 0.8829 - f1_score: 0.8853 - val_loss: 0.6845 - val_acc: 0.7931 - val_f1_score: 0.7988\n",
      "Epoch 35/60\n",
      "\n",
      "Epoch 00035: reducing learning rate to 0.0002500000118743628.\n",
      " - 40s - loss: 0.3330 - acc: 0.8897 - f1_score: 0.8916 - val_loss: 0.6845 - val_acc: 0.7999 - val_f1_score: 0.8032\n",
      "Epoch 36/60\n",
      " - 41s - loss: 0.2712 - acc: 0.9092 - f1_score: 0.9106 - val_loss: 0.6846 - val_acc: 0.8000 - val_f1_score: 0.8025\n",
      "Epoch 37/60\n",
      " - 41s - loss: 0.2600 - acc: 0.9114 - f1_score: 0.9132 - val_loss: 0.6917 - val_acc: 0.8015 - val_f1_score: 0.8060\n",
      "Epoch 38/60\n",
      " - 41s - loss: 0.2501 - acc: 0.9167 - f1_score: 0.9178 - val_loss: 0.6484 - val_acc: 0.8092 - val_f1_score: 0.8108\n",
      "Epoch 39/60\n",
      " - 42s - loss: 0.2494 - acc: 0.9146 - f1_score: 0.9157 - val_loss: 0.6789 - val_acc: 0.8047 - val_f1_score: 0.8092\n",
      "Epoch 40/60\n",
      " - 42s - loss: 0.2364 - acc: 0.9208 - f1_score: 0.9220 - val_loss: 0.6875 - val_acc: 0.8008 - val_f1_score: 0.8052\n",
      "Epoch 41/60\n",
      " - 42s - loss: 0.2264 - acc: 0.9225 - f1_score: 0.9242 - val_loss: 0.6665 - val_acc: 0.8038 - val_f1_score: 0.8096\n",
      "Epoch 42/60\n",
      " - 41s - loss: 0.2239 - acc: 0.9244 - f1_score: 0.9256 - val_loss: 0.6930 - val_acc: 0.8048 - val_f1_score: 0.8090\n",
      "Epoch 43/60\n",
      " - 41s - loss: 0.2177 - acc: 0.9269 - f1_score: 0.9276 - val_loss: 0.6772 - val_acc: 0.8072 - val_f1_score: 0.8104\n",
      "Epoch 44/60\n",
      "\n",
      "Epoch 00044: reducing learning rate to 0.0001250000059371814.\n",
      " - 41s - loss: 0.2087 - acc: 0.9290 - f1_score: 0.9296 - val_loss: 0.7127 - val_acc: 0.8013 - val_f1_score: 0.8061\n",
      "Epoch 45/60\n",
      " - 41s - loss: 0.1927 - acc: 0.9349 - f1_score: 0.9351 - val_loss: 0.6638 - val_acc: 0.8023 - val_f1_score: 0.8079\n",
      "Epoch 46/60\n",
      " - 41s - loss: 0.1813 - acc: 0.9389 - f1_score: 0.9400 - val_loss: 0.6841 - val_acc: 0.8061 - val_f1_score: 0.8110\n",
      "Epoch 47/60\n",
      " - 41s - loss: 0.1750 - acc: 0.9404 - f1_score: 0.9410 - val_loss: 0.6958 - val_acc: 0.8084 - val_f1_score: 0.8118\n",
      "Epoch 48/60\n",
      " - 41s - loss: 0.1735 - acc: 0.9404 - f1_score: 0.9419 - val_loss: 0.6852 - val_acc: 0.8073 - val_f1_score: 0.8126\n",
      "Epoch 49/60\n",
      "\n",
      "Epoch 00049: reducing learning rate to 6.25000029685907e-05.\n",
      " - 41s - loss: 0.1730 - acc: 0.9408 - f1_score: 0.9413 - val_loss: 0.6981 - val_acc: 0.8098 - val_f1_score: 0.8141\n",
      "Epoch 50/60\n",
      " - 41s - loss: 0.1590 - acc: 0.9462 - f1_score: 0.9465 - val_loss: 0.6786 - val_acc: 0.8091 - val_f1_score: 0.8132\n",
      "Epoch 51/60\n",
      " - 41s - loss: 0.1560 - acc: 0.9481 - f1_score: 0.9486 - val_loss: 0.6962 - val_acc: 0.8089 - val_f1_score: 0.8130\n",
      "Epoch 52/60\n",
      " - 41s - loss: 0.1545 - acc: 0.9475 - f1_score: 0.9481 - val_loss: 0.6920 - val_acc: 0.8128 - val_f1_score: 0.8158\n",
      "Epoch 53/60\n",
      " - 41s - loss: 0.1494 - acc: 0.9494 - f1_score: 0.9494 - val_loss: 0.6909 - val_acc: 0.8117 - val_f1_score: 0.8139\n",
      "Epoch 54/60\n",
      "\n",
      "Epoch 00054: reducing learning rate to 3.125000148429535e-05.\n",
      " - 41s - loss: 0.1524 - acc: 0.9479 - f1_score: 0.9490 - val_loss: 0.7013 - val_acc: 0.8127 - val_f1_score: 0.8167\n",
      "Epoch 55/60\n",
      " - 41s - loss: 0.1410 - acc: 0.9521 - f1_score: 0.9527 - val_loss: 0.7015 - val_acc: 0.8144 - val_f1_score: 0.8175\n",
      "Epoch 56/60\n",
      " - 40s - loss: 0.1426 - acc: 0.9520 - f1_score: 0.9523 - val_loss: 0.6945 - val_acc: 0.8139 - val_f1_score: 0.8173\n",
      "Epoch 57/60\n",
      " - 40s - loss: 0.1385 - acc: 0.9527 - f1_score: 0.9537 - val_loss: 0.6962 - val_acc: 0.8135 - val_f1_score: 0.8176\n",
      "Epoch 58/60\n",
      " - 41s - loss: 0.1383 - acc: 0.9529 - f1_score: 0.9534 - val_loss: 0.6931 - val_acc: 0.8105 - val_f1_score: 0.8144\n",
      "Epoch 59/60\n",
      "\n",
      "Epoch 00059: reducing learning rate to 1.5625000742147677e-05.\n",
      " - 41s - loss: 0.1407 - acc: 0.9516 - f1_score: 0.9523 - val_loss: 0.6996 - val_acc: 0.8138 - val_f1_score: 0.8178\n",
      "Epoch 60/60\n",
      " - 41s - loss: 0.1357 - acc: 0.9534 - f1_score: 0.9541 - val_loss: 0.6971 - val_acc: 0.8139 - val_f1_score: 0.8169\n",
      "Test loss: 0.697060049653\n",
      "Test accuracy: 0.8139\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128, (2, 2), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(128, (2, 2), activation='elu'))\n",
    "model.add(Conv2D(128, (1, 1), activation='elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(256, (2, 2), activation='elu'))\n",
    "model.add(Conv2D(256, (2, 2), activation='elu'))\n",
    "model.add(Conv2D(256, (1, 1), activation='elu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='elu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='elu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', f1_score])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=60, batch_size=32, verbose=2, callbacks=[plateau])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It should have improved slightly. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "While the Convolutional Neural Networks here perform relatively well in classifying CIFAR-10 images, it does not end here. There are a myriad of techniques and concepts that other data scientists are coming up with that either build upon CNNs or new classes of neural networks entirely. What was accomplished in this notebook is merely scratching the surface of image classification as there exists out there far better performing models than the ones I have come up with. \n",
    "\n",
    "These initial results pale in comparison to existing models, and so further research and study must be done in order to obtain a deeper understanding of the design and concept behind the aforementioned successful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "***\n",
    "Keras Documentation: \n",
    "\n",
    "https://keras.io\n",
    "\n",
    "Adit Deshpande's \"A Beginner's Guide To Understanding Convolutional Neural Networks\": \n",
    "\n",
    "https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "\n",
    "Adit Deshpande's \"The 9 Deep Learning Papers You Need to Know About\":\n",
    "\n",
    "https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\n",
    "\n",
    "Lutz Prechelt's \"Early Stopping -- but when?\": \n",
    "\n",
    "https://pdfs.semanticscholar.org/ea66/75caf8cdb9902e7889c0d75e8acc1c844b3d.pdf\n",
    "\n",
    "Simonyan & Zisserman's \"VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION\" AKA the VGG Net paper:\n",
    "\n",
    "https://arxiv.org/pdf/1409.1556v6.pdf\n",
    "\n",
    "Hadrian Lim's \"Convolutional Neural Networks with Keras\": \n",
    "\n",
    "https://github.com/hadrianpaulo/neural-nets-tutorials/blob/master/%5BL4%5D%20CNN%20with%20TF%20and%20Keras.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

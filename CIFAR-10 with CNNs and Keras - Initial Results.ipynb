{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing CIFAR-10 using Convolutional Neural Networks - Initial Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers import LocallyConnected2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the CIFAR-10 dataset anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It's a dataset that consists of 60000 32x32 colored images divided into 10 classes that correspond to 6000 images each. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/cifar10.png\\\" alt=\"CIFAR-10\\\" style=\"width: 75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our goal right now is to train a Convolutional Neural Network that could accurately classify these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize inputs from 0-255 to 0-1\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's start with a basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial model will consist of a Conv layer, followed by RELU activation layer, max pooling, dropout and then fully connected layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      " - 10s - loss: 1.6687 - acc: 0.3946 - val_loss: 1.4194 - val_acc: 0.4948\n",
      "Epoch 2/40\n",
      " - 9s - loss: 1.4165 - acc: 0.4930 - val_loss: 1.2963 - val_acc: 0.5409\n",
      "Epoch 3/40\n",
      " - 9s - loss: 1.3185 - acc: 0.5312 - val_loss: 1.2040 - val_acc: 0.5721\n",
      "Epoch 4/40\n",
      " - 9s - loss: 1.2540 - acc: 0.5575 - val_loss: 1.1486 - val_acc: 0.6000\n",
      "Epoch 5/40\n",
      " - 9s - loss: 1.2084 - acc: 0.5735 - val_loss: 1.1340 - val_acc: 0.6092\n",
      "Epoch 6/40\n",
      " - 9s - loss: 1.1716 - acc: 0.5889 - val_loss: 1.1115 - val_acc: 0.6063\n",
      "Epoch 7/40\n",
      " - 9s - loss: 1.1437 - acc: 0.5992 - val_loss: 1.0840 - val_acc: 0.6226\n",
      "Epoch 8/40\n",
      " - 9s - loss: 1.1178 - acc: 0.6051 - val_loss: 1.0702 - val_acc: 0.6252\n",
      "Epoch 9/40\n",
      " - 9s - loss: 1.0926 - acc: 0.6146 - val_loss: 1.0436 - val_acc: 0.6395\n",
      "Epoch 10/40\n",
      " - 9s - loss: 1.0763 - acc: 0.6244 - val_loss: 1.0628 - val_acc: 0.6289\n",
      "Epoch 11/40\n",
      " - 9s - loss: 1.0564 - acc: 0.6288 - val_loss: 1.0296 - val_acc: 0.6424\n",
      "Epoch 12/40\n",
      " - 9s - loss: 1.0412 - acc: 0.6338 - val_loss: 1.0448 - val_acc: 0.6367\n",
      "Epoch 13/40\n",
      " - 9s - loss: 1.0262 - acc: 0.6403 - val_loss: 1.0170 - val_acc: 0.6455\n",
      "Epoch 14/40\n",
      " - 9s - loss: 1.0153 - acc: 0.6429 - val_loss: 1.0199 - val_acc: 0.6439\n",
      "Epoch 15/40\n",
      " - 9s - loss: 1.0048 - acc: 0.6490 - val_loss: 1.0125 - val_acc: 0.6489\n",
      "Epoch 16/40\n",
      " - 9s - loss: 0.9924 - acc: 0.6541 - val_loss: 0.9920 - val_acc: 0.6514\n",
      "Epoch 17/40\n",
      " - 9s - loss: 0.9819 - acc: 0.6553 - val_loss: 1.0163 - val_acc: 0.6442\n",
      "Epoch 18/40\n",
      " - 9s - loss: 0.9721 - acc: 0.6598 - val_loss: 0.9874 - val_acc: 0.6561\n",
      "Epoch 19/40\n",
      " - 9s - loss: 0.9654 - acc: 0.6609 - val_loss: 1.0146 - val_acc: 0.6447\n",
      "Epoch 20/40\n",
      " - 9s - loss: 0.9569 - acc: 0.6646 - val_loss: 0.9756 - val_acc: 0.6600\n",
      "Epoch 21/40\n",
      " - 9s - loss: 0.9480 - acc: 0.6673 - val_loss: 0.9926 - val_acc: 0.6534\n",
      "Epoch 22/40\n",
      " - 9s - loss: 0.9459 - acc: 0.6694 - val_loss: 0.9929 - val_acc: 0.6547\n",
      "Epoch 23/40\n",
      " - 9s - loss: 0.9390 - acc: 0.6709 - val_loss: 0.9993 - val_acc: 0.6537\n",
      "Epoch 24/40\n",
      " - 9s - loss: 0.9262 - acc: 0.6755 - val_loss: 0.9951 - val_acc: 0.6512\n",
      "Epoch 25/40\n",
      " - 9s - loss: 0.9244 - acc: 0.6767 - val_loss: 0.9957 - val_acc: 0.6523\n",
      "Epoch 26/40\n",
      " - 9s - loss: 0.9230 - acc: 0.6787 - val_loss: 1.0006 - val_acc: 0.6517\n",
      "Epoch 27/40\n",
      " - 9s - loss: 0.9153 - acc: 0.6797 - val_loss: 0.9726 - val_acc: 0.6606\n",
      "Epoch 28/40\n",
      " - 9s - loss: 0.9126 - acc: 0.6775 - val_loss: 0.9677 - val_acc: 0.6631\n",
      "Epoch 29/40\n",
      " - 9s - loss: 0.9061 - acc: 0.6806 - val_loss: 0.9778 - val_acc: 0.6614\n",
      "Epoch 30/40\n",
      " - 9s - loss: 0.8980 - acc: 0.6838 - val_loss: 0.9798 - val_acc: 0.6601\n",
      "Epoch 31/40\n",
      " - 9s - loss: 0.9068 - acc: 0.6827 - val_loss: 0.9644 - val_acc: 0.6600\n",
      "Epoch 32/40\n",
      " - 9s - loss: 0.8967 - acc: 0.6835 - val_loss: 0.9600 - val_acc: 0.6618\n",
      "Epoch 33/40\n",
      " - 9s - loss: 0.8920 - acc: 0.6850 - val_loss: 0.9872 - val_acc: 0.6543\n",
      "Epoch 34/40\n",
      " - 8s - loss: 0.8947 - acc: 0.6826 - val_loss: 0.9666 - val_acc: 0.6644\n",
      "Epoch 35/40\n",
      " - 9s - loss: 0.8874 - acc: 0.6862 - val_loss: 0.9844 - val_acc: 0.6608\n",
      "Epoch 36/40\n",
      " - 9s - loss: 0.8810 - acc: 0.6887 - val_loss: 0.9788 - val_acc: 0.6596\n",
      "Epoch 37/40\n",
      " - 9s - loss: 0.8821 - acc: 0.6900 - val_loss: 0.9933 - val_acc: 0.6557\n",
      "Epoch 38/40\n",
      " - 9s - loss: 0.8709 - acc: 0.6944 - val_loss: 0.9854 - val_acc: 0.6610\n",
      "Epoch 39/40\n",
      " - 9s - loss: 0.8692 - acc: 0.6951 - val_loss: 0.9618 - val_acc: 0.6683\n",
      "Epoch 40/40\n",
      " - 9s - loss: 0.8680 - acc: 0.6953 - val_loss: 0.9583 - val_acc: 0.6728\n",
      "Test loss: 0.958309275627\n",
      "Test accuracy: 0.6728\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(8, 8)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, batch_size=32, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, ~66% accuracy isn't that bad to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at other people's successful models, we know that we could do a lot more to improve this. Let's start by adding a second convolutional layer to our initial setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      " - 12s - loss: 1.5471 - acc: 0.4348 - val_loss: 1.2380 - val_acc: 0.5626\n",
      "Epoch 2/40\n",
      " - 11s - loss: 1.2473 - acc: 0.5553 - val_loss: 1.1151 - val_acc: 0.6101\n",
      "Epoch 3/40\n",
      " - 12s - loss: 1.1401 - acc: 0.5962 - val_loss: 1.0650 - val_acc: 0.6228\n",
      "Epoch 4/40\n",
      " - 12s - loss: 1.0721 - acc: 0.6213 - val_loss: 1.0016 - val_acc: 0.6466\n",
      "Epoch 5/40\n",
      " - 11s - loss: 1.0178 - acc: 0.6400 - val_loss: 0.9782 - val_acc: 0.6649\n",
      "Epoch 6/40\n",
      " - 11s - loss: 0.9794 - acc: 0.6551 - val_loss: 0.9355 - val_acc: 0.6703\n",
      "Epoch 7/40\n",
      " - 12s - loss: 0.9459 - acc: 0.6669 - val_loss: 0.9405 - val_acc: 0.6686\n",
      "Epoch 8/40\n",
      " - 12s - loss: 0.9194 - acc: 0.6722 - val_loss: 0.9121 - val_acc: 0.6844\n",
      "Epoch 9/40\n",
      " - 11s - loss: 0.8969 - acc: 0.6845 - val_loss: 0.9115 - val_acc: 0.6799\n",
      "Epoch 10/40\n",
      " - 11s - loss: 0.8761 - acc: 0.6896 - val_loss: 0.8961 - val_acc: 0.6869\n",
      "Epoch 11/40\n",
      " - 11s - loss: 0.8533 - acc: 0.6973 - val_loss: 0.8878 - val_acc: 0.6903\n",
      "Epoch 12/40\n",
      " - 11s - loss: 0.8393 - acc: 0.7023 - val_loss: 0.8832 - val_acc: 0.6863\n",
      "Epoch 13/40\n",
      " - 11s - loss: 0.8261 - acc: 0.7073 - val_loss: 0.8519 - val_acc: 0.7047\n",
      "Epoch 14/40\n",
      " - 11s - loss: 0.8119 - acc: 0.7138 - val_loss: 0.8736 - val_acc: 0.6953\n",
      "Epoch 15/40\n",
      " - 12s - loss: 0.8041 - acc: 0.7132 - val_loss: 0.8798 - val_acc: 0.6963\n",
      "Epoch 16/40\n",
      " - 12s - loss: 0.7938 - acc: 0.7187 - val_loss: 0.8607 - val_acc: 0.7012\n",
      "Epoch 17/40\n",
      " - 12s - loss: 0.7817 - acc: 0.7216 - val_loss: 0.8616 - val_acc: 0.7024\n",
      "Epoch 18/40\n",
      " - 12s - loss: 0.7680 - acc: 0.7279 - val_loss: 0.8652 - val_acc: 0.7039\n",
      "Epoch 19/40\n",
      " - 12s - loss: 0.7603 - acc: 0.7283 - val_loss: 0.8421 - val_acc: 0.7117\n",
      "Epoch 20/40\n",
      " - 12s - loss: 0.7521 - acc: 0.7319 - val_loss: 0.8500 - val_acc: 0.7053\n",
      "Epoch 21/40\n",
      " - 12s - loss: 0.7458 - acc: 0.7363 - val_loss: 0.8625 - val_acc: 0.7043\n",
      "Epoch 22/40\n",
      " - 12s - loss: 0.7393 - acc: 0.7374 - val_loss: 0.8350 - val_acc: 0.7110\n",
      "Epoch 23/40\n",
      " - 12s - loss: 0.7341 - acc: 0.7382 - val_loss: 0.8442 - val_acc: 0.7032\n",
      "Epoch 24/40\n",
      " - 11s - loss: 0.7252 - acc: 0.7421 - val_loss: 0.8595 - val_acc: 0.7049\n",
      "Epoch 25/40\n",
      " - 12s - loss: 0.7199 - acc: 0.7442 - val_loss: 0.8517 - val_acc: 0.7059\n",
      "Epoch 26/40\n",
      " - 12s - loss: 0.7166 - acc: 0.7471 - val_loss: 0.8308 - val_acc: 0.7145\n",
      "Epoch 27/40\n",
      " - 12s - loss: 0.7042 - acc: 0.7506 - val_loss: 0.8285 - val_acc: 0.7129\n",
      "Epoch 28/40\n",
      " - 12s - loss: 0.7076 - acc: 0.7479 - val_loss: 0.8449 - val_acc: 0.7110\n",
      "Epoch 29/40\n",
      " - 12s - loss: 0.6933 - acc: 0.7529 - val_loss: 0.8718 - val_acc: 0.7013\n",
      "Epoch 30/40\n",
      " - 12s - loss: 0.6922 - acc: 0.7550 - val_loss: 0.8389 - val_acc: 0.7152\n",
      "Epoch 31/40\n",
      " - 12s - loss: 0.6895 - acc: 0.7552 - val_loss: 0.8419 - val_acc: 0.7109\n",
      "Epoch 32/40\n",
      " - 12s - loss: 0.6881 - acc: 0.7541 - val_loss: 0.8581 - val_acc: 0.7060\n",
      "Epoch 33/40\n",
      " - 12s - loss: 0.6848 - acc: 0.7561 - val_loss: 0.8431 - val_acc: 0.7112\n",
      "Epoch 34/40\n",
      " - 12s - loss: 0.6770 - acc: 0.7592 - val_loss: 0.8583 - val_acc: 0.7107\n",
      "Epoch 35/40\n",
      " - 12s - loss: 0.6716 - acc: 0.7613 - val_loss: 0.8379 - val_acc: 0.7177\n",
      "Epoch 36/40\n",
      " - 12s - loss: 0.6693 - acc: 0.7615 - val_loss: 0.8503 - val_acc: 0.7144\n",
      "Epoch 37/40\n",
      " - 12s - loss: 0.6628 - acc: 0.7634 - val_loss: 0.8414 - val_acc: 0.7132\n",
      "Epoch 38/40\n",
      " - 11s - loss: 0.6708 - acc: 0.7614 - val_loss: 0.8358 - val_acc: 0.7148\n",
      "Epoch 39/40\n",
      " - 11s - loss: 0.6588 - acc: 0.7683 - val_loss: 0.8477 - val_acc: 0.7128\n",
      "Epoch 40/40\n",
      " - 12s - loss: 0.6556 - acc: 0.7658 - val_loss: 0.8466 - val_acc: 0.7175\n",
      "Test loss: 0.84663061285\n",
      "Test accuracy: 0.7175\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(8, 8)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, batch_size=32, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Oh wow! An improvement of ~5%! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*But we can do better. We have the technology.*\n",
    "\n",
    "Notice that our validation accuracy has stopped improving at around the 25th-30th epoch and has only been fluctuating slightly around its peak value with no drastic change. We're wasting resources by continuing to train the model with no significant improvement. At worst, it can lead to overfitting and may actually pull our model's performance down.\n",
    "\n",
    "---\n",
    "\n",
    "Introducting the concept of **Early Stopping**.\n",
    "\n",
    "We can have the model stop training after an arbitrary amount of epochs with no improvement, thus saving some valuable time and resources and preventing the model from overfitting on the training data. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we Early Stop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, this is what a training error vs. validation error graph would look like. You would want to stop training when we reach the validation error's global minimum. Simple, right? Just stop training the instant validation loss goes up!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/ideal.png\\\" alt=\"CIFAR-10\\\" style=\"width: 35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, this is almost never the case, as in reality error scores can fluctuate and create multiple local minima such as this example. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/real.png\\\" alt=\"CIFAR-10\\\" style=\"width: 35%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why Early Stopping is a thing, and it has multiple criteria for *when* to actually stop.\n",
    "***\n",
    "We won't go in detail about that in this notebook. For now, let us add **Early Stopping** to our model's callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estop = EarlyStopping(monitor='val_loss', min_delta=0, patience=4, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration of Early Stopping checks the validation loss at every epoch. If there has been no improvement for 4 epochs (dictated by the 'patience' argument), it stops training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      " - 12s - loss: 1.5829 - acc: 0.4217 - val_loss: 1.2885 - val_acc: 0.5465\n",
      "Epoch 2/40\n",
      " - 11s - loss: 1.2990 - acc: 0.5344 - val_loss: 1.1511 - val_acc: 0.6000\n",
      "Epoch 3/40\n",
      " - 11s - loss: 1.1952 - acc: 0.5723 - val_loss: 1.0682 - val_acc: 0.6252\n",
      "Epoch 4/40\n",
      " - 12s - loss: 1.1243 - acc: 0.5995 - val_loss: 1.0459 - val_acc: 0.6382\n",
      "Epoch 5/40\n",
      " - 12s - loss: 1.0777 - acc: 0.6181 - val_loss: 1.0082 - val_acc: 0.6495\n",
      "Epoch 6/40\n",
      " - 12s - loss: 1.0330 - acc: 0.6334 - val_loss: 1.0154 - val_acc: 0.6414\n",
      "Epoch 7/40\n",
      " - 12s - loss: 1.0071 - acc: 0.6422 - val_loss: 0.9762 - val_acc: 0.6629\n",
      "Epoch 8/40\n",
      " - 12s - loss: 0.9828 - acc: 0.6522 - val_loss: 0.9306 - val_acc: 0.6739\n",
      "Epoch 9/40\n",
      " - 12s - loss: 0.9631 - acc: 0.6588 - val_loss: 0.9595 - val_acc: 0.6672\n",
      "Epoch 10/40\n",
      " - 12s - loss: 0.9473 - acc: 0.6653 - val_loss: 0.9290 - val_acc: 0.6771\n",
      "Epoch 11/40\n",
      " - 12s - loss: 0.9292 - acc: 0.6692 - val_loss: 0.9288 - val_acc: 0.6725\n",
      "Epoch 12/40\n",
      " - 12s - loss: 0.9179 - acc: 0.6766 - val_loss: 0.9083 - val_acc: 0.6837\n",
      "Epoch 13/40\n",
      " - 11s - loss: 0.9035 - acc: 0.6824 - val_loss: 0.9174 - val_acc: 0.6816\n",
      "Epoch 14/40\n",
      " - 11s - loss: 0.8884 - acc: 0.6843 - val_loss: 0.8915 - val_acc: 0.6904\n",
      "Epoch 15/40\n",
      " - 11s - loss: 0.8792 - acc: 0.6879 - val_loss: 0.9038 - val_acc: 0.6876\n",
      "Epoch 16/40\n",
      " - 11s - loss: 0.8756 - acc: 0.6888 - val_loss: 0.9017 - val_acc: 0.6869\n",
      "Epoch 17/40\n",
      " - 12s - loss: 0.8630 - acc: 0.6956 - val_loss: 0.8977 - val_acc: 0.6924\n",
      "Epoch 18/40\n",
      " - 12s - loss: 0.8537 - acc: 0.6985 - val_loss: 0.8998 - val_acc: 0.6889\n",
      "Test loss: 0.899753879929\n",
      "Test accuracy: 0.6889\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(8, 8)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, batch_size=32, verbose=2, callbacks=[estop])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We should've gotten similar results as the last model we trained. Only this time, our model went through significantly less epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a possibility that our accuracy has dipped in comparison to the previous model which may indicate *too* early stopping.\n",
    "\n",
    "Anyway, we need to go *deeper.*\n",
    "\n",
    "Let's add more conv+pool layers! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      " - 15s - loss: 1.6939 - acc: 0.3665 - val_loss: 1.5357 - val_acc: 0.4431\n",
      "Epoch 2/40\n",
      " - 14s - loss: 1.3891 - acc: 0.4976 - val_loss: 1.2999 - val_acc: 0.5273\n",
      "Epoch 3/40\n",
      " - 14s - loss: 1.2485 - acc: 0.5529 - val_loss: 1.2420 - val_acc: 0.5572\n",
      "Epoch 4/40\n",
      " - 14s - loss: 1.1509 - acc: 0.5883 - val_loss: 1.1126 - val_acc: 0.6066\n",
      "Epoch 5/40\n",
      " - 14s - loss: 1.0743 - acc: 0.6188 - val_loss: 1.0664 - val_acc: 0.6213\n",
      "Epoch 6/40\n",
      " - 14s - loss: 1.0200 - acc: 0.6382 - val_loss: 1.0508 - val_acc: 0.6270\n",
      "Epoch 7/40\n",
      " - 14s - loss: 0.9762 - acc: 0.6535 - val_loss: 0.9780 - val_acc: 0.6546\n",
      "Epoch 8/40\n",
      " - 14s - loss: 0.9380 - acc: 0.6691 - val_loss: 1.0065 - val_acc: 0.6519\n",
      "Epoch 9/40\n",
      " - 14s - loss: 0.9041 - acc: 0.6785 - val_loss: 1.0618 - val_acc: 0.6389\n",
      "Epoch 10/40\n",
      " - 14s - loss: 0.8791 - acc: 0.6891 - val_loss: 0.9801 - val_acc: 0.6603\n",
      "Epoch 11/40\n",
      " - 14s - loss: 0.8557 - acc: 0.6972 - val_loss: 0.9632 - val_acc: 0.6677\n",
      "Epoch 12/40\n",
      " - 14s - loss: 0.8365 - acc: 0.7017 - val_loss: 0.9316 - val_acc: 0.6772\n",
      "Epoch 13/40\n",
      " - 14s - loss: 0.8123 - acc: 0.7120 - val_loss: 0.9568 - val_acc: 0.6634\n",
      "Epoch 14/40\n",
      " - 14s - loss: 0.8009 - acc: 0.7180 - val_loss: 0.9464 - val_acc: 0.6750\n",
      "Epoch 15/40\n",
      " - 14s - loss: 0.7801 - acc: 0.7222 - val_loss: 0.9351 - val_acc: 0.6785\n",
      "Epoch 16/40\n",
      " - 14s - loss: 0.7619 - acc: 0.7300 - val_loss: 0.9767 - val_acc: 0.6608\n",
      "Test loss: 0.97667738781\n",
      "Test accuracy: 0.6608\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model.add(Conv2D(32, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, batch_size=32, verbose=2, callbacks=[estop])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What? I thought putting more layers in will automagically give us better results??"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all the time, especially if you don't know what you're doing. Notice that, while the filter sizes for the conv layers were retained, the pool's filter sizes had to be adjusted in order to accomodate these extra conv layers. As we know, due to the nature of how convolution works, our output volume shrinks after every conv layer. On top of that, the output volume gets downsampled through the use of the max pooling layer. \n",
    "\n",
    "<img src=\"figs/maxpool.png\\\" alt=\"CIFAR-10\\\" style=\"width: 45%\">\n",
    "\n",
    "***\n",
    "\n",
    "**Downsampling** isn't inherently a bad thing. In fact, it is really useful in what we're doing. First, it gives us less to work with, therefore making our data computationally more manageable. Second, it prevents overfitting, especially in the context of pooling, by generalizing certain regions in the image. \n",
    "\n",
    "***\n",
    "In this case though, the output volume has become too small that it is possible that we have lost some valuable information that would be vital in classification.  \n",
    "\n",
    "Let us attempt to remedy that by making our conv filters smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      " - 14s - loss: 1.4696 - acc: 0.4639 - val_loss: 1.2370 - val_acc: 0.5586\n",
      "Epoch 2/40\n",
      " - 13s - loss: 1.1015 - acc: 0.6078 - val_loss: 1.0066 - val_acc: 0.6496\n",
      "Epoch 3/40\n",
      " - 13s - loss: 0.9458 - acc: 0.6666 - val_loss: 0.8772 - val_acc: 0.6888\n",
      "Epoch 4/40\n",
      " - 13s - loss: 0.8543 - acc: 0.6994 - val_loss: 0.8257 - val_acc: 0.7134\n",
      "Epoch 5/40\n",
      " - 13s - loss: 0.7793 - acc: 0.7281 - val_loss: 0.8259 - val_acc: 0.7140\n",
      "Epoch 6/40\n",
      " - 13s - loss: 0.7301 - acc: 0.7434 - val_loss: 0.7918 - val_acc: 0.7211\n",
      "Epoch 7/40\n",
      " - 13s - loss: 0.6881 - acc: 0.7590 - val_loss: 0.7565 - val_acc: 0.7413\n",
      "Epoch 8/40\n",
      " - 13s - loss: 0.6558 - acc: 0.7694 - val_loss: 0.7646 - val_acc: 0.7374\n",
      "Epoch 9/40\n",
      " - 13s - loss: 0.6186 - acc: 0.7802 - val_loss: 0.7409 - val_acc: 0.7523\n",
      "Epoch 10/40\n",
      " - 13s - loss: 0.5912 - acc: 0.7901 - val_loss: 0.7722 - val_acc: 0.7452\n",
      "Epoch 11/40\n",
      " - 13s - loss: 0.5615 - acc: 0.8032 - val_loss: 0.7316 - val_acc: 0.7538\n",
      "Epoch 12/40\n",
      " - 13s - loss: 0.5402 - acc: 0.8082 - val_loss: 0.7511 - val_acc: 0.7532\n",
      "Epoch 13/40\n",
      " - 13s - loss: 0.5180 - acc: 0.8170 - val_loss: 0.7468 - val_acc: 0.7517\n",
      "Epoch 14/40\n",
      " - 13s - loss: 0.4966 - acc: 0.8226 - val_loss: 0.7826 - val_acc: 0.7423\n",
      "Epoch 15/40\n",
      " - 13s - loss: 0.4818 - acc: 0.8266 - val_loss: 0.8229 - val_acc: 0.7411\n",
      "Test loss: 0.822910208035\n",
      "Test accuracy: 0.7411\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, batch_size=32, verbose=2, callbacks=[estop])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It works! Plus, it's accuracy has improved by ~4%! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What now?**\n",
    "We can attempt to squeeze all the improvement possible from our current model because there is a chance that early stopping prematurely halts training before finding our loss's global minimum. \n",
    "***\n",
    "Introducing the **ReduceLROnPlateau** callback! \n",
    "***\n",
    "Instead of outright stopping our model's training, we can just reduce its learning rate so we can see less erratic loss and accuracy scores. Similar conditions from Early Stopping apply, as we only reduce LR after a certain number of epochs without improvement.\n",
    "\n",
    "Let's replace early stopping for now and add LR reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plateau = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This configuration for LR reduction halves the learning rate after 5 epochs that exhibit no improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      " - 14s - loss: 1.5069 - acc: 0.4520 - val_loss: 1.3220 - val_acc: 0.5278\n",
      "Epoch 2/40\n",
      " - 13s - loss: 1.1569 - acc: 0.5888 - val_loss: 1.0333 - val_acc: 0.6376\n",
      "Epoch 3/40\n",
      " - 13s - loss: 0.9856 - acc: 0.6529 - val_loss: 0.9336 - val_acc: 0.6706\n",
      "Epoch 4/40\n",
      " - 13s - loss: 0.8808 - acc: 0.6904 - val_loss: 0.8648 - val_acc: 0.6931\n",
      "Epoch 5/40\n",
      " - 13s - loss: 0.8046 - acc: 0.7163 - val_loss: 0.8572 - val_acc: 0.7023\n",
      "Epoch 6/40\n",
      " - 13s - loss: 0.7455 - acc: 0.7347 - val_loss: 0.7943 - val_acc: 0.7232\n",
      "Epoch 7/40\n",
      " - 13s - loss: 0.7036 - acc: 0.7532 - val_loss: 0.8055 - val_acc: 0.7242\n",
      "Epoch 8/40\n",
      " - 13s - loss: 0.6662 - acc: 0.7654 - val_loss: 0.8130 - val_acc: 0.7173\n",
      "Epoch 9/40\n",
      " - 13s - loss: 0.6374 - acc: 0.7726 - val_loss: 0.7660 - val_acc: 0.7359\n",
      "Epoch 10/40\n",
      " - 13s - loss: 0.6021 - acc: 0.7861 - val_loss: 0.7484 - val_acc: 0.7434\n",
      "Epoch 11/40\n",
      " - 13s - loss: 0.5771 - acc: 0.7937 - val_loss: 0.7657 - val_acc: 0.7403\n",
      "Epoch 12/40\n",
      " - 13s - loss: 0.5548 - acc: 0.8051 - val_loss: 0.7645 - val_acc: 0.7416\n",
      "Epoch 13/40\n",
      " - 13s - loss: 0.5312 - acc: 0.8123 - val_loss: 0.8267 - val_acc: 0.7339\n",
      "Epoch 14/40\n",
      " - 13s - loss: 0.5095 - acc: 0.8184 - val_loss: 0.8209 - val_acc: 0.7319\n",
      "Epoch 15/40\n",
      " - 13s - loss: 0.4950 - acc: 0.8226 - val_loss: 0.8083 - val_acc: 0.7426\n",
      "Epoch 16/40\n",
      "\n",
      "Epoch 00016: reducing learning rate to 0.0005000000237487257.\n",
      " - 13s - loss: 0.4728 - acc: 0.8327 - val_loss: 0.8144 - val_acc: 0.7384\n",
      "Epoch 17/40\n",
      " - 13s - loss: 0.3833 - acc: 0.8630 - val_loss: 0.8038 - val_acc: 0.7548\n",
      "Epoch 18/40\n",
      " - 13s - loss: 0.3620 - acc: 0.8674 - val_loss: 0.8335 - val_acc: 0.7523\n",
      "Epoch 19/40\n",
      " - 13s - loss: 0.3542 - acc: 0.8738 - val_loss: 0.8257 - val_acc: 0.7543\n",
      "Epoch 20/40\n",
      " - 13s - loss: 0.3357 - acc: 0.8797 - val_loss: 0.8542 - val_acc: 0.7518\n",
      "Epoch 21/40\n",
      "\n",
      "Epoch 00021: reducing learning rate to 0.0002500000118743628.\n",
      " - 13s - loss: 0.3258 - acc: 0.8820 - val_loss: 0.8749 - val_acc: 0.7495\n",
      "Epoch 22/40\n",
      " - 13s - loss: 0.2779 - acc: 0.9002 - val_loss: 0.8724 - val_acc: 0.7542\n",
      "Epoch 23/40\n",
      " - 13s - loss: 0.2676 - acc: 0.9034 - val_loss: 0.8798 - val_acc: 0.7586\n",
      "Epoch 24/40\n",
      " - 13s - loss: 0.2589 - acc: 0.9058 - val_loss: 0.9008 - val_acc: 0.7541\n",
      "Epoch 25/40\n",
      " - 13s - loss: 0.2536 - acc: 0.9094 - val_loss: 0.8970 - val_acc: 0.7538\n",
      "Epoch 26/40\n",
      "\n",
      "Epoch 00026: reducing learning rate to 0.0001250000059371814.\n",
      " - 13s - loss: 0.2476 - acc: 0.9105 - val_loss: 0.9147 - val_acc: 0.7586\n",
      "Epoch 27/40\n",
      " - 13s - loss: 0.2241 - acc: 0.9183 - val_loss: 0.9297 - val_acc: 0.7572\n",
      "Epoch 28/40\n",
      " - 13s - loss: 0.2168 - acc: 0.9223 - val_loss: 0.9452 - val_acc: 0.7593\n",
      "Epoch 29/40\n",
      " - 13s - loss: 0.2117 - acc: 0.9223 - val_loss: 0.9438 - val_acc: 0.7554\n",
      "Epoch 30/40\n",
      " - 13s - loss: 0.2109 - acc: 0.9246 - val_loss: 0.9434 - val_acc: 0.7543\n",
      "Epoch 31/40\n",
      "\n",
      "Epoch 00031: reducing learning rate to 6.25000029685907e-05.\n",
      " - 13s - loss: 0.2066 - acc: 0.9256 - val_loss: 0.9558 - val_acc: 0.7556\n",
      "Epoch 32/40\n",
      " - 13s - loss: 0.1955 - acc: 0.9298 - val_loss: 0.9528 - val_acc: 0.7567\n",
      "Epoch 33/40\n",
      " - 13s - loss: 0.1927 - acc: 0.9309 - val_loss: 0.9627 - val_acc: 0.7564\n",
      "Epoch 34/40\n",
      " - 13s - loss: 0.1950 - acc: 0.9296 - val_loss: 0.9690 - val_acc: 0.7555\n",
      "Epoch 35/40\n",
      " - 13s - loss: 0.1890 - acc: 0.9327 - val_loss: 0.9660 - val_acc: 0.7584\n",
      "Epoch 36/40\n",
      "\n",
      "Epoch 00036: reducing learning rate to 3.125000148429535e-05.\n",
      " - 13s - loss: 0.1876 - acc: 0.9319 - val_loss: 0.9750 - val_acc: 0.7585\n",
      "Epoch 37/40\n",
      " - 13s - loss: 0.1836 - acc: 0.9344 - val_loss: 0.9741 - val_acc: 0.7584\n",
      "Epoch 38/40\n",
      " - 13s - loss: 0.1823 - acc: 0.9340 - val_loss: 0.9714 - val_acc: 0.7568\n",
      "Epoch 39/40\n",
      " - 13s - loss: 0.1797 - acc: 0.9354 - val_loss: 0.9833 - val_acc: 0.7577\n",
      "Epoch 40/40\n",
      " - 13s - loss: 0.1823 - acc: 0.9354 - val_loss: 0.9818 - val_acc: 0.7566\n",
      "Test loss: 0.981845797443\n",
      "Test accuracy: 0.7566\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, batch_size=32, verbose=2, callbacks=[plateau])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another improvement! The catch was that we had to go through all the epochs since we took out early stopping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try improving our model further by adding classic fully connected layers.\n",
    "\n",
    "Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/40\n",
      " - 16s - loss: 1.6464 - acc: 0.3869 - val_loss: 1.3098 - val_acc: 0.5208\n",
      "Epoch 2/40\n",
      " - 15s - loss: 1.2633 - acc: 0.5486 - val_loss: 1.1010 - val_acc: 0.6106\n",
      "Epoch 3/40\n",
      " - 15s - loss: 1.1080 - acc: 0.6097 - val_loss: 1.0092 - val_acc: 0.6528\n",
      "Epoch 4/40\n",
      " - 15s - loss: 0.9942 - acc: 0.6513 - val_loss: 0.9077 - val_acc: 0.6881\n",
      "Epoch 5/40\n",
      " - 15s - loss: 0.9317 - acc: 0.6734 - val_loss: 0.8613 - val_acc: 0.7021\n",
      "Epoch 6/40\n",
      " - 15s - loss: 0.8757 - acc: 0.6926 - val_loss: 0.8565 - val_acc: 0.7071\n",
      "Epoch 7/40\n",
      " - 15s - loss: 0.8399 - acc: 0.7062 - val_loss: 0.8239 - val_acc: 0.7183\n",
      "Epoch 8/40\n",
      " - 15s - loss: 0.8045 - acc: 0.7177 - val_loss: 0.8129 - val_acc: 0.7212\n",
      "Epoch 9/40\n",
      " - 15s - loss: 0.7830 - acc: 0.7261 - val_loss: 0.7823 - val_acc: 0.7301\n",
      "Epoch 10/40\n",
      " - 15s - loss: 0.7562 - acc: 0.7359 - val_loss: 0.7993 - val_acc: 0.7289\n",
      "Epoch 11/40\n",
      " - 15s - loss: 0.7365 - acc: 0.7415 - val_loss: 0.7497 - val_acc: 0.7463\n",
      "Epoch 12/40\n",
      " - 15s - loss: 0.7147 - acc: 0.7505 - val_loss: 0.7505 - val_acc: 0.7425\n",
      "Epoch 13/40\n",
      " - 15s - loss: 0.7010 - acc: 0.7545 - val_loss: 0.7654 - val_acc: 0.7371\n",
      "Epoch 14/40\n",
      " - 15s - loss: 0.6851 - acc: 0.7604 - val_loss: 0.7802 - val_acc: 0.7361\n",
      "Epoch 15/40\n",
      " - 15s - loss: 0.6735 - acc: 0.7645 - val_loss: 0.7556 - val_acc: 0.7434\n",
      "Epoch 16/40\n",
      " - 15s - loss: 0.6554 - acc: 0.7693 - val_loss: 0.7172 - val_acc: 0.7567\n",
      "Epoch 17/40\n",
      " - 15s - loss: 0.6438 - acc: 0.7741 - val_loss: 0.7767 - val_acc: 0.7396\n",
      "Epoch 18/40\n",
      " - 15s - loss: 0.6378 - acc: 0.7770 - val_loss: 0.7422 - val_acc: 0.7480\n",
      "Epoch 19/40\n",
      " - 15s - loss: 0.6260 - acc: 0.7821 - val_loss: 0.7947 - val_acc: 0.7378\n",
      "Epoch 20/40\n",
      " - 15s - loss: 0.6233 - acc: 0.7812 - val_loss: 0.7768 - val_acc: 0.7385\n",
      "Epoch 21/40\n",
      " - 15s - loss: 0.6122 - acc: 0.7861 - val_loss: 0.7617 - val_acc: 0.7481\n",
      "Epoch 22/40\n",
      "\n",
      "Epoch 00022: reducing learning rate to 0.0005000000237487257.\n",
      " - 15s - loss: 0.6061 - acc: 0.7890 - val_loss: 0.7314 - val_acc: 0.7533\n",
      "Epoch 23/40\n",
      " - 15s - loss: 0.5245 - acc: 0.8137 - val_loss: 0.7210 - val_acc: 0.7631\n",
      "Epoch 24/40\n",
      " - 15s - loss: 0.5071 - acc: 0.8219 - val_loss: 0.7103 - val_acc: 0.7633\n",
      "Epoch 25/40\n",
      " - 15s - loss: 0.4966 - acc: 0.8244 - val_loss: 0.7163 - val_acc: 0.7625\n",
      "Epoch 26/40\n",
      " - 15s - loss: 0.4831 - acc: 0.8277 - val_loss: 0.7241 - val_acc: 0.7656\n",
      "Epoch 27/40\n",
      " - 15s - loss: 0.4766 - acc: 0.8320 - val_loss: 0.7119 - val_acc: 0.7641\n",
      "Epoch 28/40\n",
      " - 15s - loss: 0.4718 - acc: 0.8315 - val_loss: 0.7201 - val_acc: 0.7648\n",
      "Epoch 29/40\n",
      " - 15s - loss: 0.4704 - acc: 0.8329 - val_loss: 0.7231 - val_acc: 0.7669\n",
      "Epoch 30/40\n",
      "\n",
      "Epoch 00030: reducing learning rate to 0.0002500000118743628.\n",
      " - 15s - loss: 0.4581 - acc: 0.8384 - val_loss: 0.7465 - val_acc: 0.7578\n",
      "Epoch 31/40\n",
      " - 15s - loss: 0.4211 - acc: 0.8500 - val_loss: 0.7187 - val_acc: 0.7692\n",
      "Epoch 32/40\n",
      " - 15s - loss: 0.4089 - acc: 0.8549 - val_loss: 0.7266 - val_acc: 0.7684\n",
      "Epoch 33/40\n",
      " - 15s - loss: 0.4055 - acc: 0.8545 - val_loss: 0.7274 - val_acc: 0.7717\n",
      "Epoch 34/40\n",
      " - 15s - loss: 0.3965 - acc: 0.8557 - val_loss: 0.7257 - val_acc: 0.7733\n",
      "Epoch 35/40\n",
      "\n",
      "Epoch 00035: reducing learning rate to 0.0001250000059371814.\n",
      " - 15s - loss: 0.3933 - acc: 0.8584 - val_loss: 0.7288 - val_acc: 0.7703\n",
      "Epoch 36/40\n",
      " - 15s - loss: 0.3720 - acc: 0.8675 - val_loss: 0.7333 - val_acc: 0.7745\n",
      "Epoch 37/40\n",
      " - 15s - loss: 0.3697 - acc: 0.8676 - val_loss: 0.7377 - val_acc: 0.7749\n",
      "Epoch 38/40\n",
      " - 16s - loss: 0.3713 - acc: 0.8653 - val_loss: 0.7318 - val_acc: 0.7727\n",
      "Epoch 39/40\n",
      " - 16s - loss: 0.3682 - acc: 0.8706 - val_loss: 0.7341 - val_acc: 0.7747\n",
      "Epoch 40/40\n",
      "\n",
      "Epoch 00040: reducing learning rate to 6.25000029685907e-05.\n",
      " - 16s - loss: 0.3620 - acc: 0.8712 - val_loss: 0.7482 - val_acc: 0.7734\n",
      "Test loss: 0.748225223207\n",
      "Test accuracy: 0.7734\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=40, batch_size=32, verbose=2, callbacks=[plateau])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It should have improved slightly. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "While the Convolutional Neural Networks here perform relatively well in classifying CIFAR-10 images, it does not end here. There are a myriad of techniques and concepts that other data scientists are coming up with that either build upon CNNs or new classes of neural networks entirely. What was accomplished in this notebook is merely scratching the surface of image classification as there exists out there far better performing models than the ones I have come up with. \n",
    "\n",
    "These initial results pale in comparison to existing models, and so further research and study must be done in order to obtain a deeper understanding of the design and concept behind the aforementioned successful models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "***\n",
    "Keras Documentation: \n",
    "\n",
    "https://keras.io\n",
    "\n",
    "Adit Deshpande's \"A Beginner's Guide To Understanding Convolutional Neural Networks\": \n",
    "\n",
    "https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "\n",
    "Adit Deshpande's \"The 9 Deep Learning Papers You Need to Know About\":\n",
    "\n",
    "https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html\n",
    "\n",
    "Lutz Prechelt's \"Early Stopping -- but when?\": \n",
    "\n",
    "https://pdfs.semanticscholar.org/ea66/75caf8cdb9902e7889c0d75e8acc1c844b3d.pdf\n",
    "\n",
    "Simonyan & Zisserman's \"VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION\" AKA the VGG Net paper:\n",
    "\n",
    "https://arxiv.org/pdf/1409.1556v6.pdf\n",
    "\n",
    "Hadrian Lim's \"Convolutional Neural Networks with Keras\": \n",
    "\n",
    "https://github.com/hadrianpaulo/neural-nets-tutorials/blob/master/%5BL4%5D%20CNN%20with%20TF%20and%20Keras.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
